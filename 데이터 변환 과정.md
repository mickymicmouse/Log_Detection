# 데이터 변환 과정 설명

### HDFS

* 하둡 파일 시스템 데이터
* 500,000개의 데이터가 있으며, 라벨링된 데이터이기 때문에 대표적인 벤치마킹 데이터
* return 값

| line | log                                                          |
| ---- | ------------------------------------------------------------ |
| 1    | 081109 203615 148 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_38865049064139660 terminating |
| 2    | 081109 203807 222 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-6952295868487656571 terminating |
| 3    | 081109 204005 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_7128370237687728475 size 67108864 |
| 4    | 081109 204015 308 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8229193803249955061 terminating |
| 5    | 081109 204106 329 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6670958622368987959 terminating |



### Drain

* Log parser로 좋은 성능을 나타냄
* EventID 생성
* [Drain 논문](https://jiemingzhu.github.io/pub/pjhe_icws2017.pdf)
* HDFS Raw Log Data -> Structured data
* Return 값

| LineId | Date  | Time   | Pid  | Level | Component                | Content                                                      | EventId | EventTemplate                            |
| ------ | ----- | ------ | ---- | ----- | ------------------------ | ------------------------------------------------------------ | ------- | ---------------------------------------- |
| 1      | 81109 | 203518 | 143  | INFO  | dfs.DataNode$DataXceiver | Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest:  /10.250.19.102:50010 | E5      | Receiving block <*> src: /<*> dest: /<*> |
| 2      | 81109 | 203518 | 35   | INFO  | dfs.FSNamesystem         | BLOCK* NameSystem.allocateBlock:  /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar.  blk_-1608999687919862906 | E22     | BLOCK* NameSystem.allocateBlock:<*>      |
| 3      | 81109 | 203519 | 143  | INFO  | dfs.DataNode$DataXceiver | Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest:  /10.250.10.6:50010 | E5      | Receiving block <*> src: /<*> dest: /<*> |
| 4      | 81109 | 203519 | 145  | INFO  | dfs.DataNode$DataXceiver | Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest:  /10.250.14.224:50010 | E5      | Receiving block <*> src: /<*> dest: /<*> |



### Sampling with BlockID (Sequencing)

* structured data의 event sequence(이벤트 발생 순서) 를 content의 블록아이디 별로 구성

```python
  
import os 
import re
import numpy as np 
import pandas as pd
from collections import OrderedDict

def hdfs_sampling(log_file, window='session', window_size=0):
    assert window == 'session', "Only window=session is supported for HDFS dataset."
    print("Loading", log_file)
    struct_log = pd.read_csv(log_file, engine='c',
            na_filter=False, memory_map=True)
    data_dict = OrderedDict()
    for idx, row in struct_log.iterrows():
        blkId_list = re.findall(r'(blk_-?\d+)', row['Content'])
        blkId_set = set(blkId_list)
        for blk_Id in blkId_set:
            if not blk_Id in data_dict:
                data_dict[blk_Id] = []
            data_dict[blk_Id].append(row['EventId'])
    data_df = pd.DataFrame(list(data_dict.items()), columns=['BlockId', 'EventSequence'])
    data_df.to_csv("hdfs/HDFS_sequence.csv",index=None)

hdfs_sampling('hdfs/HDFS_100k.log_structured.csv')
```

* return 값

| BlockId                  | EventSequence                                                |
| ------------------------ | ------------------------------------------------------------ |
| blk_-1608999687919862906 | ['E5', 'E22', 'E5', 'E5', 'E11', 'E11', 'E9', 'E9', 'E11', 'E9', 'E26',  'E26', 'E26', 'E6', 'E5', 'E16', 'E6', 'E5', 'E18', 'E25', 'E26', 'E26',  'E3', 'E25', 'E6', 'E6', 'E5', 'E5', 'E16', 'E18', 'E26', 'E26', 'E5', 'E6',  'E5', 'E16', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E18', 'E25', 'E6', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E26', 'E26', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E25', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E18', 'E6', 'E5', 'E3', 'E3', 'E3', 'E3', 'E3', 'E16', 'E3',  'E3', 'E3', 'E3', 'E26', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3'] |
| blk_7503483334202473044  | ['E5', 'E5', 'E22', 'E5', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26',  'E26', 'E26', 'E3', 'E2', 'E2'] |
| blk_-3544583377289625738 | ['E5', 'E22', 'E5', 'E5', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E3',  'E26', 'E26', 'E26', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3',  'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3', 'E3'] |
| blk_-9073992586687739851 | ['E5', 'E22', 'E5', 'E5', 'E11', 'E9', 'E11', 'E9', 'E11', 'E9', 'E26',  'E26', 'E26', 'E2', 'E2', 'E2'] |



### Log Entry 분류

* HDFS 의 경우
  * 세션 창 기반
* BGL 의 경우
  * 시간 스탬프 기반(고정 윈도우, 슬라이딩 윈도우) 

주어진 입력 B의 재구성 오류를 기반으로 이상 징후 점수를 계산하기 위한 마스크로 사용한다. 

피쳐 추출

이 단계의 주요 목적은 이상 탐지 모델에 공급될 수 있는 로그 이벤트에서 중요한 기능을 추출하는 것이다. 기능 추출의 입력은 로그 구문 분석 단계에서 생성된 로그 이벤트이며 출력은 이벤트 수 매트릭스입니다. 특징을 추출하기 위해서는 먼저 로그 데이터를 다양한 그룹으로 분리해야 하며, 여기서 각 그룹은 로그 시퀀스를 나타낸다. 이를 위해 로그 데이터 세트를 유한 청크로 나누는 윈도우 설정이 적용됩니다 [5]. 그림 1에 표시된 것처럼 고정 창, 슬라이딩 창, 세션 창 등 세 가지 유형의 창을 사용합니다.

고정 창: 고정 창과 슬라이딩 창은 모두 각 로그의 발생 시간을 기록하는 타임스탬프를 기반으로 합니다. 각 고정 창의 크기는 시간 범위 또는 기간을 의미합니다. 그림 1과 같이 창 크기는 1시간 또는 1일과 같은 상수 값인 µt입니다. 따라서 고정 창의 수는 미리 정의된 창 크기에 따라 달라집니다. 동일한 창에서 발생한 로그는 로그 시퀀스로 간주됩니다.

슬라이딩 윈도우: 고정 윈도우와는 달리 슬라이딩 윈도우는 윈도우 크기와 스텝 크기, 예를 들어 매 5분마다 한 번씩 윈도우를 슬라이딩하는 두 가지 특성으로 구성됩니다. 일반적으로 단계 크기가 창 크기보다 작으므로 서로 다른 창이 겹치게 됩니다. 그림 1은 윈도우 크기가 ΔT인 반면 스텝 크기는 포워딩 거리이다. 고정 윈도우보다 종종 큰 슬라이딩 윈도우의 수는 주로 윈도우 크기와 스텝 크기에 따라 달라집니다. 동일한 슬라이딩 윈도우에서 발생한 로그도 로그 시퀀스로 그룹화되지만 겹침으로 인해 여러 슬라이딩 윈도우에서 로그가 중복될 수 있습니다.

세션 창: 위의 두 가지 창 유형과 비교하여 세션 창은 타임스탬프 대신 식별자를 기반으로 합니다. 식별자는 일부 로그 데이터에서 서로 다른 실행 경로를 표시하는 데 사용됩니다. 예를 들어 block_id가 있는 HDFS 로그는 특정 블록의 할당, 쓰기, 복제, 삭제를 기록합니다. 따라서, 우리는 각 세션 창에 고유한 식별자가 있는 식별자에 따라 로그를 그룹화할 수 있다.

윈도우 설정 기법으로 로그 시퀀스를 구성하면 이벤트 카운트 매트릭스 X가 생성됩니다. 각 로그 순서에서, 우리는 이벤트 카운트 벡터를 형성하기 위해 각 로그 이벤트의 발생 수를 카운트한다. 예를 들어, 이벤트 카운트 벡터가 [0, 0, 2, 3, 0, 1, 0]이면 이 로그 시퀀스에서 이벤트 3이 두 번, 이벤트 4가 세 번 발생했음을 의미합니다. 마지막으로, 많은 이벤트 카운트 벡터는 이벤트 카운트 매트릭스 X가 되도록 구성되며, 여기서 엔트리 X는 i번째 로그 시퀀스에서 이벤트 j가 발생한 횟수를 기록합니다.